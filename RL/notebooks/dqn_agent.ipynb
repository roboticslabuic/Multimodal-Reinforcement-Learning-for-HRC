{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GdVSmFuegWDi"
   },
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"The DQN agent that interacts with the user.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of DQNAgent.\n",
    "\n",
    "        The constructor of DQNAgent which saves constants, sets up neural network graphs, etc.\n",
    "\n",
    "        \"\"\"\n",
    "        self.memory = []\n",
    "        self.balanced_memory = []\n",
    "        self.num_samples_epoch = 0\n",
    "        self.memory_index = 0\n",
    "        self.max_memory_size = 20000 #200000, 125000, 10000,5000? #last stable version 15000\n",
    "        self.eps = 0.2 #0.5*(0.9**5) #0.5*(0.9**2)\n",
    "        self.lr = 1e-6 #1e-8, 1e-6, 1e-3\n",
    "        self.gamma = 0.9 #0.9, 0.5\n",
    "        self.batch_size = 32 #16, 32\n",
    "        self.hidden_size_1 = 32 #32\n",
    "        self.hidden_size_2 = 8   #8\n",
    "        self.model_num = 0\n",
    "        \n",
    "        with open('../hel_rule_data', 'r') as f:\n",
    "              self.rule_data = json.load(f)\n",
    "        #self.len_rule_datapoint = 6\n",
    "\n",
    "        self.load_weights_file_path = \"../dqn_models\"\n",
    "        self.save_weights_file_path = \"../dqn_models\"\n",
    "\n",
    "        if self.max_memory_size < self.batch_size:\n",
    "            raise ValueError('Max memory size must be at least as great as batch size!')\n",
    "\n",
    "        self.state_size = 26 #the input size to dqn_agent model. \n",
    "                             #the size of the state=[actor, hel_action, hel_da, eld_action, eld_da, hel_ot,hel_l,hel_o]\n",
    "                             #size_state=[1,8,13,6,13,2,2,2] --> 41\n",
    "                             #the size of the state=[actor, eld_action, eld_da, hel_ot,hel_l,hel_o]\n",
    "                             #size_state=[1,6,13,2,2,2] --> 26\n",
    "                        \n",
    "        self.num_action = 9 #the number of helper's actions 0-8\n",
    "        self.num_da = 14 #the number of helper's das 0-13\n",
    "\n",
    "\n",
    "        self.beh_model = self._build_model()\n",
    "        self.tar_model = self._build_model()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.huber = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "        self.mae = nn.L1Loss()\n",
    "\n",
    "        #self.beh_optimizer = torch.optim.SGD(self.beh_model.parameters(), lr=self.lr, momentum=0.9)\n",
    "        #self.tar_optimizer = torch.optim.SGD(self.tar_model.parameters(), lr=self.lr, momentum=0.9)\n",
    "        self.beh_optimizer = torch.optim.Adam(self.beh_model.parameters(), lr=self.lr)\n",
    "        self.tar_optimizer = torch.optim.Adam(self.tar_model.parameters(), lr=self.lr)\n",
    "\n",
    "        self._load_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds and returns model/graph of neural network.\"\"\"\n",
    "\n",
    "        model = helper_model(self.hidden_size_1,self.hidden_size_2, self.state_size,\n",
    "                             self.num_action).to(device)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, agent_input, elder_output, helper_ot_l_o, warmup):\n",
    "        \"\"\"\n",
    "        Returns the action of the agent given a state.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.eps > random.random():\n",
    "            agent_output = {}\n",
    "            #agent_output['da_out'] = torch.Tensor([0]*self.num_da).to(device=device)\n",
    "            #index_da = random.randint(0, self.num_da - 1)\n",
    "            #agent_output['da_out'][index_da] = 1\n",
    "            \n",
    "            agent_output['action_out'] = torch.Tensor([0]*self.num_action).to(device=device)\n",
    "            index_action = random.randint(0, self.num_action - 1)\n",
    "            agent_output['action_out'][index_action] = 1\n",
    "\n",
    "            return agent_output\n",
    "        else:\n",
    "            if warmup:\n",
    "                return self._rule_action(elder_output, helper_ot_l_o)\n",
    "            else:\n",
    "                return self._dqn_action(agent_input)\n",
    "\n",
    "    def prep_input(self, elder_output, helper_ot_l_o):\n",
    "        helper_input_ot = torch.Tensor(2*[0]).to(device=device)\n",
    "        helper_input_l = torch.Tensor(2*[0]).to(device=device)        \n",
    "        helper_input_o = torch.Tensor(2*[0]).to(device=device)\n",
    "        \n",
    "        helper_input_actor = torch.Tensor(1*[0]).to(device=device)\n",
    "        helper_input_eld_action = torch.Tensor(6*[0]).to(device=device)\n",
    "        helper_input_eld_da = torch.Tensor(13*[0]).to(device=device)\n",
    "\n",
    "        if helper_ot_l_o[0][1] == 1:\n",
    "            helper_input_ot[0] = 1\n",
    "        elif helper_ot_l_o[0][1] == 2:\n",
    "            helper_input_ot[1] = 1\n",
    "            \n",
    "        if helper_ot_l_o[1][1] == 1:\n",
    "            helper_input_l[0] = 1\n",
    "        elif helper_ot_l_o[1][1] == 2:\n",
    "            helper_input_l[1] = 1\n",
    "\n",
    "        if helper_ot_l_o[2][1] == 1:\n",
    "            helper_input_o[0] = 1\n",
    "        elif helper_ot_l_o[2][1] == 2:\n",
    "            helper_input_o[1] = 1\n",
    "                \n",
    "        if elder_output['action_out']:\n",
    "            helper_input_eld_action[elder_output['action_out']-1] = 1\n",
    "        else:\n",
    "            helper_input_actor = torch.Tensor(1*[1]).to(device=device)\n",
    "            \n",
    "        if elder_output['da_out']:\n",
    "            helper_input_eld_da[elder_output['da_out']-1] = 1\n",
    "\n",
    "        \n",
    "        helper_input = torch.cat((helper_input_ot, helper_input_l, helper_input_o,\n",
    "                                  helper_input_actor, helper_input_eld_action, helper_input_eld_da), 0)\n",
    "        \n",
    "        return helper_input\n",
    "\n",
    "    def _rule_action(self, elder_output, helper_ot_l_o):\n",
    "        #print(elder_output, helper_ot_l_o)\n",
    "        hel_action_out = {}\n",
    "        hel_action_out['action_out'] = torch.Tensor(9*[0]).to(device=device)\n",
    "        indx__ = [0]\n",
    "        for datapoint in self.rule_data:\n",
    "            #print(elder_output['action_out'].item(), datapoint['eld_action'])\n",
    "            if (elder_output['da_out'].item() == datapoint['eld_da'] and\n",
    "                elder_output['action_out'].item() == datapoint['eld_action'] and\n",
    "                helper_ot_l_o[0][1] == datapoint['ot'] and helper_ot_l_o[1][1] == datapoint['l'] \n",
    "                and helper_ot_l_o[2][1] == datapoint['o']):\n",
    "                \n",
    "                indx__.append(datapoint['hel_action'])\n",
    "                \n",
    "        #print(indx__)\n",
    "        exp_flag = False\n",
    "        if indx__ != [0]:\n",
    "            exp_flag = True\n",
    "        indx_ = indx__[random.randint(0,len(indx__)-1)]        \n",
    "        hel_action_out['action_out'][indx_] = 1\n",
    "        #print(hel_action_out)\n",
    "        return hel_action_out\n",
    "    \n",
    "    def get_expert_action(self, elder_output, helper_ot_l_o):\n",
    "        #print(elder_output, helper_ot_l_o)\n",
    "        hel_action_out = {}\n",
    "        hel_action_out['action_out'] = torch.Tensor(9*[0]).to(device=device)\n",
    "        indx__ = [0]\n",
    "        for datapoint in self.rule_data:\n",
    "            #print(elder_output['action_out'].item(), datapoint['eld_action'])\n",
    "            if (elder_output['da_out'].item() == datapoint['eld_da'] and\n",
    "                elder_output['action_out'].item() == datapoint['eld_action'] and\n",
    "                helper_ot_l_o[0][1] == datapoint['ot'] and helper_ot_l_o[1][1] == datapoint['l'] \n",
    "                and helper_ot_l_o[2][1] == datapoint['o']):\n",
    "                \n",
    "                indx__.append(datapoint['hel_action'])\n",
    "                \n",
    "        #print(indx__)\n",
    "        exp_flag = False\n",
    "        if indx__ != [0]:\n",
    "            exp_flag = True\n",
    "        indx_ = indx__[random.randint(0,len(indx__)-1)]        \n",
    "        hel_action_out['action_out'][indx_] = 1\n",
    "        #print(hel_action_out)\n",
    "        return hel_action_out, exp_flag\n",
    "                \n",
    "      \n",
    "\n",
    "    def _dqn_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns a behavior model output given a state.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_output = self._dqn_predict_one(state)\n",
    "        return agent_output\n",
    "\n",
    "\n",
    "    def _dqn_predict_one(self, state, target=False):\n",
    "        agent_output = self._dqn_predict(state, target)\n",
    "        return agent_output\n",
    "\n",
    "    def _dqn_predict(self, states, target=False):\n",
    "        \"\"\"\n",
    "        Returns a model prediction given an array of states.\n",
    "\n",
    "        \"\"\"\n",
    "        if target:\n",
    "            return self.tar_model(states)\n",
    "        else:\n",
    "            return self.beh_model(states)\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, done, success):\n",
    "        \"\"\"\n",
    "        Adds an experience made of the parameters to the memory.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) < self.max_memory_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.memory_index] = (state, action, reward, next_state, done, success)\n",
    "        self.memory_index = (self.memory_index + 1) % self.max_memory_size\n",
    "\n",
    "        \n",
    "        \n",
    "    def empty_memory(self):\n",
    "        \"\"\"Empties the memory and resets the memory index.\"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.balanced_memory = []\n",
    "        self.memory_index = 0\n",
    "\n",
    "    def is_memory_full(self):\n",
    "        \"\"\"Returns true if the memory is full.\"\"\"\n",
    "\n",
    "        return len(self.memory) == self.max_memory_size\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent by improving the behavior model given the memory tuples.\n",
    "\n",
    "        Takes batches of memories from the memory pool and processing them. The processing takes the tuples and stacks\n",
    "        them in the correct format for the neural network and calculates the Bellman equation for Q-Learning.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calc. num of batches to run\n",
    "        num_batches = len(self.memory) // self.batch_size\n",
    "        print(\"num_batches=\", num_batches)\n",
    "        batchlosses_of_oneepoch = [] # each element is the loss of one batch\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            #for sample in batch:\n",
    "              #print(sample)\n",
    "\n",
    "            states = [sample[0] for sample in batch]\n",
    "            next_states = [sample[3] for sample in batch]\n",
    "\n",
    "            beh_state_preds = []\n",
    "            for state in states:\n",
    "                beh_state_preds.append(self._dqn_predict_one(self.prep_input(state[0], state[1][1])))\n",
    "            \n",
    "\n",
    "            #if not self.vanilla:\n",
    "             #   beh_next_states_preds = []  # For indexing for DDQN\n",
    "              #  for state in next_states:\n",
    "               #     beh_next_states_preds.append(self._dqn_predict(self.prep_input(state[0], state[1])))\n",
    "            \n",
    "\n",
    "            tar_next_state_preds = []\n",
    "            for state in next_states:\n",
    "                tar_next_state_preds.append(self._dqn_predict(self.prep_input(state[0], state[1][1]), target=True))  # For target value for DQN (& DDQN)\n",
    "            \n",
    "\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            \n",
    "\n",
    "            for i, (s, a, r, s_, d, success) in enumerate(batch):\n",
    "                t = beh_state_preds[i]                                    \n",
    "                #t['da_out'] = r + self.gamma * tar_next_state_preds[i]['da_out']\n",
    "                t['action_out'] = r + self.gamma * (tar_next_state_preds[i]['action_out'])*(not success)\n",
    "                \n",
    "                inputs.append(self.prep_input(s[0],s[1][1]))\n",
    "                targets.append(t)\n",
    "\n",
    "            #das_t = [targets[j][\"da_out\"] for j in range(len(targets))]\n",
    "            actions_t = [targets[j][\"action_out\"] for j in range(len(targets))]\n",
    "            total_train_loss = 0\n",
    "            self.beh_model.train()            \n",
    "            for i in range(len(inputs)):\n",
    "                to_input = inputs[i]\n",
    "                #da_t = das_t[i]\n",
    "                action_t = actions_t[i]\n",
    "                self.beh_model.zero_grad()\n",
    "                self.beh_optimizer.zero_grad()\n",
    "                to_input = to_input.to(device=device)\n",
    "                output = self.beh_model(to_input)\n",
    "                loss = torch.zeros(1).to(device=device)\n",
    "                loss += self.mae(output[\"action_out\"], action_t.to(device=device))\n",
    "                #loss += self.mae(output[\"da_out\"], da_t.to(device=device))\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.beh_optimizer.step()\n",
    "\n",
    "            batchlosses_of_oneepoch.append( total_train_loss)\n",
    "\n",
    "        print(\"Training loss:\", np.mean(np.asarray(batchlosses_of_oneepoch)))\n",
    "        return batchlosses_of_oneepoch\n",
    "\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Copies the behavior model's weights into the target model's weights.\"\"\"\n",
    "        self.tar_model.load_state_dict(self.beh_model.state_dict())\n",
    "        #self.tar_model.eval() #??????????????????????????????????????????\n",
    "        torch.save(self.tar_model.state_dict(), '../dqn_models/tar_model.pt')\n",
    "        self.tar_model.to(device=device)\n",
    "        #self.tar_model.eval() #??????????????????????????????????????????\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"Saves the weights of both models in two files.\"\"\"\n",
    "        if not self.save_weights_file_path:\n",
    "            return\n",
    "        beh_save_file_path = self.save_weights_file_path\n",
    "        #self.beh_model.eval() #??????????????????????????????????????????\n",
    "        torch.save(self.beh_model.state_dict(), beh_save_file_path + '/beh_model_'+ str(self.model_num) +'.pt')\n",
    "        self.load_weights_file_path = self.save_weights_file_path\n",
    "\n",
    "\n",
    "    def _load_weights(self):\n",
    "        \"\"\"Loads the weights of both models from two h5 files.\"\"\"\n",
    "\n",
    "        if not self.load_weights_file_path:\n",
    "            return\n",
    "        beh_load_file_path = self.load_weights_file_path\n",
    "        self.beh_model.load_state_dict(torch.load(beh_load_file_path + '/beh_model.pt'))\n",
    "        self.beh_model.to(device=device)\n",
    "        #self.beh_model.eval()\n",
    "\n",
    "        tar_load_file_path = self.load_weights_file_path        \n",
    "        self.tar_model.load_state_dict(torch.load(tar_load_file_path + '/tar_model.pt'))\n",
    "        self.tar_model.to(device=device)\n",
    "        #self.tar_model.eval()\n",
    "        \n",
    "\n",
    "class helper_model(nn.Module):\n",
    "    def __init__(self, size1, size2, num_in, num_action_out):\n",
    "        super(helper_model, self).__init__()\n",
    "        self._size1 = size1\n",
    "        self._size2 = size2\n",
    "        self._layer1 = nn.Linear(num_in, size1)\n",
    "        self._layer2 = nn.Linear(size1, size2)\n",
    "        self._dropout = nn.Dropout(p=0.1)\n",
    "        #self._da_out = nn.Linear(size2, num_da_out)\n",
    "        self._action_out = nn.Linear(size2, num_action_out)\n",
    "\n",
    "    def forward(self, feature_input):\n",
    "        layer1_out = self._layer1(feature_input)\n",
    "        layer2_out = self._layer2(layer1_out)\n",
    "        hidden_out = self._dropout(layer2_out)\n",
    "        to_return = {}\n",
    "        #to_return[\"da_out\"] = F.relu(self._da_out(hidden_out))\n",
    "        to_return[\"action_out\"] = F.relu(self._action_out(hidden_out))\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgentNB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
