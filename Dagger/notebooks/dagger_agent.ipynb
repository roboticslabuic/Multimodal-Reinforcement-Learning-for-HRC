{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdVSmFuegWDi"
   },
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DAGGER_Agent:\n",
    "    \"\"\"The DQN agent that interacts with the user.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of DQNAgent.\n",
    "\n",
    "        The constructor of DQNAgent which saves constants, sets up neural network graphs, etc.\n",
    "\n",
    "        \"\"\"\n",
    "        self.memory = []\n",
    "        self.num_samples_epoch = 0\n",
    "        self.memory_index = 0\n",
    "        self.max_memory_size = 10000 #200000, 125000, 10000,5000? #last stable version 15000\n",
    "        self.lr = 1e-4 #1e-8, 1e-6, 1e-3\n",
    "        self.batch_size = 32 #16, 32, 8\n",
    "        self.hidden_size_1 = 32 #32\n",
    "        self.hidden_size_2 = 8   #8\n",
    "        self.model_num = 0\n",
    "        \n",
    "        with open('../rule_data.json', 'r') as f:\n",
    "              self.rule_data = json.load(f)\n",
    "        #self.len_rule_datapoint = 6\n",
    "\n",
    "        self.load_weights_file_path = \"\"\n",
    "        self.save_weights_file_path = \"../dagger_models\"\n",
    "\n",
    "        if self.max_memory_size < self.batch_size:\n",
    "            raise ValueError('Max memory size must be at least as great as batch size!')\n",
    "\n",
    "        self.state_size = 26 #the input size to dqn_agent model. \n",
    "                             #the size of the state=[actor, hel_action, hel_da, eld_action, eld_da, hel_ot,hel_l,hel_o]\n",
    "                             #size_state=[1,8,13,6,13,2,2,2] --> 41\n",
    "                             #the size of the state=[actor, eld_action, eld_da, hel_ot,hel_l,hel_o]\n",
    "                             #size_state=[1,6,13,2,2,2] --> 26\n",
    "                        \n",
    "        self.num_action = 9 #the number of helper's actions 0-8\n",
    "        self.num_da = 14 #the number of helper's das 0-13\n",
    "\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.huber = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "        self.mae = nn.L1Loss()\n",
    "\n",
    "        #self.beh_optimizer = torch.optim.SGD(self.beh_model.parameters(), lr=self.lr, momentum=0.9)\n",
    "        #self.tar_optimizer = torch.optim.SGD(self.tar_model.parameters(), lr=self.lr, momentum=0.9)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self._load_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds and returns model/graph of neural network.\"\"\"\n",
    "\n",
    "        model = helper_model(self.hidden_size_1,self.hidden_size_2, self.state_size,\n",
    "                             self.num_action).to(device)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, agent_input, elder_output, helper_ot_l_o, warmup):\n",
    "        \"\"\"\n",
    "        Returns the action of the agent given a state.\n",
    "\n",
    "        \"\"\"\n",
    "        if warmup:\n",
    "            return self._rule_action(elder_output, helper_ot_l_o)\n",
    "        else:\n",
    "            return self._dagger_action(agent_input)\n",
    "        \n",
    "    def get_expert_action(self, agent_input, elder_output, helper_ot_l_o):\n",
    "        \"\"\"\n",
    "        Returns the action of the agent given a state.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._rule_action(elder_output, helper_ot_l_o)\n",
    "\n",
    "\n",
    "    def prep_input(self, elder_output, helper_ot_l_o):\n",
    "        helper_input_ot = torch.Tensor(2*[0]).to(device=device)\n",
    "        helper_input_l = torch.Tensor(2*[0]).to(device=device)        \n",
    "        helper_input_o = torch.Tensor(2*[0]).to(device=device)\n",
    "        \n",
    "        helper_input_actor = torch.Tensor(1*[0]).to(device=device)\n",
    "        helper_input_eld_action = torch.Tensor(6*[0]).to(device=device)\n",
    "        helper_input_eld_da = torch.Tensor(13*[0]).to(device=device)\n",
    "\n",
    "        if helper_ot_l_o[0][1] == 1:\n",
    "            helper_input_ot[0] = 1\n",
    "        elif helper_ot_l_o[0][1] == 2:\n",
    "            helper_input_ot[1] = 1\n",
    "            \n",
    "        if helper_ot_l_o[1][1] == 1:\n",
    "            helper_input_l[0] = 1\n",
    "        elif helper_ot_l_o[1][1] == 2:\n",
    "            helper_input_l[1] = 1\n",
    "\n",
    "        if helper_ot_l_o[2][1] == 1:\n",
    "            helper_input_o[0] = 1\n",
    "        elif helper_ot_l_o[2][1] == 2:\n",
    "            helper_input_o[1] = 1\n",
    "                \n",
    "        if elder_output['action_out']:\n",
    "            helper_input_eld_action[elder_output['action_out']-1] = 1\n",
    "        else:\n",
    "            helper_input_actor = torch.Tensor(1*[1]).to(device=device)\n",
    "            \n",
    "        if elder_output['da_out']:\n",
    "            #elder_output['da_out']=elder_output['da_out'].item()\n",
    "            #if elder_output['da_out']==6 and elder_output['action_out'] in [1,2,3]:\n",
    "                #elder_output['da_out']= 1\n",
    "            helper_input_eld_da[elder_output['da_out']-1] = 1\n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "        helper_input = torch.cat((helper_input_ot, helper_input_l, helper_input_o,\n",
    "                                  helper_input_actor, helper_input_eld_action, helper_input_eld_da), 0)\n",
    "        \n",
    "        return helper_input\n",
    "\n",
    "    def _rule_action(self, elder_output, helper_ot_l_o):\n",
    "        #print(elder_output, helper_ot_l_o)\n",
    "        hel_action_out = {}\n",
    "        hel_action_out['action_out'] = torch.Tensor(9*[0]).to(device=device)\n",
    "        indx__ = [0]\n",
    "        for datapoint in self.rule_data:\n",
    "            #print(elder_output['action_out'].item(), datapoint['eld_action'])\n",
    "            if (elder_output['da_out'].item() == datapoint['eld_da'] and\n",
    "                elder_output['action_out'].item() == datapoint['eld_action'] and\n",
    "                helper_ot_l_o[0][1] == datapoint['ot'] and helper_ot_l_o[1][1] == datapoint['l'] \n",
    "                and helper_ot_l_o[2][1] == datapoint['o']):\n",
    "                \n",
    "                if datapoint['hel_action']==2 and datapoint['hel_action']==1:\n",
    "                    print(datapoint['ot'],datapoint['l'],datapoint['o'])\n",
    "                    if datapoint['ot']==1 and datapoint['l']==1 and datapoint['o']==0:\n",
    "                        print(datapoint['hel_action'])\n",
    "                \n",
    "                indx__.append(datapoint['hel_action'])\n",
    "                \n",
    "        #print(indx__)\n",
    "        exp_flag = False\n",
    "        if indx__ != [0]:\n",
    "            exp_flag = True\n",
    "        #indx_ = indx__[random.randint(0,len(indx__)-1)]\n",
    "        indx_ = max(set(indx__), key = indx__.count)\n",
    "        if len(indx__)>=2 and indx_==0:\n",
    "            indx_=indx__[1]\n",
    "            \n",
    "\n",
    "        hel_action_out['action_out'][indx_] = 1\n",
    "        #print(hel_action_out)\n",
    "        return hel_action_out, exp_flag\n",
    "                \n",
    "      \n",
    "\n",
    "    def _dagger_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns a behavior model output given a state.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_output = self._dagger_predict_one(state)\n",
    "        return agent_output\n",
    "\n",
    "\n",
    "    def _dagger_predict_one(self, state):\n",
    "        agent_output = self._dagger_predict(state)\n",
    "        return agent_output\n",
    "\n",
    "    def _dagger_predict(self, states):\n",
    "        \"\"\"\n",
    "        Returns a model prediction given an array of states.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.model(states)\n",
    "\n",
    "    def add_experience(self, state, action, expert_action, done, success):\n",
    "        \"\"\"\n",
    "        Adds an experience made of the parameters to the memory.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) < self.max_memory_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.memory_index] = (state, action, expert_action, done, success)\n",
    "        self.memory_index = (self.memory_index + 1) % self.max_memory_size\n",
    "\n",
    "        \n",
    "        \n",
    "    def empty_memory(self):\n",
    "        \"\"\"Empties the memory and resets the memory index.\"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.balanced_memory = []\n",
    "        self.memory_index = 0\n",
    "\n",
    "    def is_memory_full(self):\n",
    "        \"\"\"Returns true if the memory is full.\"\"\"\n",
    "\n",
    "        return len(self.memory) == self.max_memory_size\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent by improving the behavior model given the memory tuples.\n",
    "\n",
    "        Takes batches of memories from the memory pool and processing them. The processing takes the tuples and stacks\n",
    "        them in the correct format for the neural network and calculates the Bellman equation for Q-Learning.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calc. num of batches to run\n",
    "        num_batches = len(self.memory) // self.batch_size\n",
    "        print(\"num_batches=\", num_batches)\n",
    "        batchlosses_of_oneepoch = [] # each element is the loss of one batch\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            #print('--'*20)\n",
    "            #for sample in batch:\n",
    "            #    print(sample)\n",
    "            #print(len(batch))\n",
    "            states = [sample[0] for sample in batch]\n",
    "            actions = [sample[1] for sample in batch]\n",
    "            expert_actions = [sample[2] for sample in batch]\n",
    "            \n",
    "\n",
    "            inputs = []\n",
    "            labels = []\n",
    "            \n",
    "\n",
    "            for i, (s, a, e_a, d, success) in enumerate(batch):\n",
    "                t = expert_actions[i]                                 \n",
    "                labels.append(t)                \n",
    "                inputs.append(self.prep_input(s[0],s[1][1]))\n",
    "\n",
    "\n",
    "            #das_t = [targets[j][\"da_out\"] for j in range(len(targets))]\n",
    "            actions_t = [labels[j][\"action_out\"] for j in range(len(labels))]\n",
    "            #print( len(actions_t), actions_t)\n",
    "            total_train_loss = 0\n",
    "            self.model.train()            \n",
    "            for i in range(len(inputs)):                \n",
    "                to_input = inputs[i]\n",
    "                to_input = to_input.to(device=device)\n",
    "                #da_t = das_t[i]\n",
    "                action_t = actions_t[i]\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                output = self.model(to_input)\n",
    "                \n",
    "                loss = torch.zeros(1).to(device=device)\n",
    "                #loss += self.mse(output[\"action_out\"], action_t.to(device=device))\n",
    "                #loss += self.mae(output[\"da_out\"], da_t.to(device=device))\n",
    "                __pred__ = torch.unsqueeze( output[\"action_out\"], dim=0)\n",
    "                __target__ = torch.as_tensor( torch.argmax( action_t.to(device=device)), device=device).unsqueeze(0)\n",
    "                loss += self.ce( __pred__, __target__)\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            batchlosses_of_oneepoch.append( total_train_loss)\n",
    "\n",
    "        print(\"Training loss:\", np.mean(np.asarray(batchlosses_of_oneepoch)))\n",
    "        return batchlosses_of_oneepoch\n",
    "\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"Saves the weights of both models in two files.\"\"\"\n",
    "        if not self.save_weights_file_path:\n",
    "            return\n",
    "        save_file_path = self.save_weights_file_path\n",
    "        #self.beh_model.eval() #??????????????????????????????????????????\n",
    "        torch.save(self.model.state_dict(), save_file_path + '/dagger_model_'+ str(self.model_num) +'.pt')\n",
    "        self.load_weights_file_path = self.save_weights_file_path\n",
    "\n",
    "\n",
    "    def _load_weights(self):\n",
    "        \"\"\"Loads the weights of both models from two h5 files.\"\"\"\n",
    "\n",
    "        if not self.load_weights_file_path:\n",
    "            return\n",
    "        load_file_path = self.load_weights_file_path\n",
    "        self.model.load_state_dict(torch.load(load_file_path + '/dagger_model.pt'))\n",
    "        self.model.to(device=device)\n",
    "        #self.beh_model.eval()\n",
    "\n",
    "        \n",
    "\n",
    "class helper_model(nn.Module):\n",
    "    def __init__(self, size1, size2, num_in, num_action_out):\n",
    "        super(helper_model, self).__init__()\n",
    "        self._size1 = size1\n",
    "        self._size2 = size2\n",
    "        self._layer1 = nn.Linear(num_in, size1)\n",
    "        self._layer2 = nn.Linear(size1, size2)\n",
    "        self._dropout = nn.Dropout(p=0.2)\n",
    "        #self._da_out = nn.Linear(size2, num_da_out)\n",
    "        self._action_out = nn.Linear(size2, num_action_out)\n",
    "\n",
    "    def forward(self, feature_input):\n",
    "        layer1_out = self._layer1(feature_input)\n",
    "        layer2_out = self._layer2(layer1_out)\n",
    "        hidden_out = self._dropout(layer2_out)\n",
    "        to_return = {}\n",
    "        #to_return[\"da_out\"] = F.relu(self._da_out(hidden_out))\n",
    "        to_return[\"action_out\"] = F.relu(self._action_out(hidden_out))\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQNAgentNB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
